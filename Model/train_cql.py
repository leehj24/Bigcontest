# Model/train_cql.py  (d3rlpy ìµœì‹  í˜¸í™˜: no logdir/with_timestamp/experiment_name)
import os
from pathlib import Path
import numpy as np
import pandas as pd
from d3rlpy.algos import DiscreteCQL, DiscreteCQLConfig
from d3rlpy.dataset import MDPDataset
from sklearn.preprocessing import StandardScaler
import joblib
import time

ROOT = Path(__file__).resolve().parents[1]   # Bigcontest
MODEL_DIR = ROOT / "Model"
LOGS_ROOT = MODEL_DIR / "d3rlpy_logs"        # d3rlpy ê¸°ë³¸ ë¡œê·¸ëŠ” ./d3rlpy_logs ê¸°ì¤€ (ì‘ì—… ë””ë ‰í„°ë¦¬)
LOGS_ROOT.mkdir(parents=True, exist_ok=True)

def echo(msg): print(f"[train_cql] {msg}")

# 0) ì…ë ¥ íŒŒì¼ í™•ì¸
rl_csv = MODEL_DIR / "rl_dataset.csv"
if not rl_csv.exists():
    raise FileNotFoundError(f"í›ˆë ¨ìš© RL ë°ì´í„° ì—†ìŒ: {rl_csv}")

echo(f"ë¡œë“œ: {rl_csv}")
df = pd.read_csv(rl_csv)
echo(f"í–‰ ê°œìˆ˜: {len(df):,}")

need_latent = [f"latent_{i}" for i in range(16)]
need_feat = ["DLV_SAA_RAT","MCT_UE_CLN_REU_RAT","MCT_UE_CLN_NEW_RAT","RC_M1_SAA_RANK","TREND_RATIO"]
need = ["action","reward","terminal"] + need_latent + need_feat
miss = [c for c in need if c not in df.columns]
if miss:
    raise ValueError(f"ì»¬ëŸ¼ ëˆ„ë½: {miss}")

# ë°ì´í„° í†µê³„
acts = df["action"].astype(int)
uniq_acts, counts = np.unique(acts, return_counts=True)
echo(f"ì•¡ì…˜ ë¶„í¬: " + ", ".join([f"{a}:{c}" for a,c in zip(uniq_acts, counts)]))
echo(f"í„°ë¯¸ë„=1 ê°œìˆ˜: {int(df['terminal'].sum()):,}")
echo(f"ë³´ìƒ í†µê³„: mean={df['reward'].mean():.4f} std={df['reward'].std():.4f} min={df['reward'].min():.4f} max={df['reward'].max():.4f}")

# ì•ˆì „ ê°€ë“œ
if len(df) < 500:
    raise RuntimeError(f"ë°ì´í„°ê°€ ë„ˆë¬´ ì ìŠµë‹ˆë‹¤(len={len(df)}). rl_dataset.csv íŒŒì´í”„ë¼ì¸ ì ê²€ í•„ìš”.")
if len(uniq_acts) < 2:
    raise RuntimeError("ì•¡ì…˜ì´ í•œ ì¢…ë¥˜ë¿ì…ë‹ˆë‹¤. rule_action/ë°ì´í„° ë¶„í¬ ì ê²€ í•„ìš”.")

# ê´€ì¸¡ êµ¬ì„±
X_latent = df[need_latent].astype(np.float32)
X_feats  = df[need_feat].astype(np.float32)

# ìŠ¤ì¼€ì¼ëŸ¬ (ì›”ì§€í‘œ 5ê°œë§Œ)
scaler_path = MODEL_DIR / "standard_scaler.pkl"
if scaler_path.exists():
    scaler = joblib.load(scaler_path)
    echo(f"ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ: {scaler_path.name}")
else:
    scaler = StandardScaler().fit(X_feats)
    joblib.dump(scaler, scaler_path)
    echo(f"ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ… ë° ì €ì¥: {scaler_path.name}")

X_feats_scaled = scaler.transform(X_feats)
obs = np.concatenate([X_latent.values, X_feats_scaled.astype(np.float32)], axis=1).astype(np.float32)
echo(f"ê´€ì¸¡ ì°¨ì› í™•ì¸: obs.shape={obs.shape} (ê¸°ëŒ€ 21)")

actions   = acts.values
rewards   = df["reward"].astype(np.float32).values
terminals = df["terminal"].astype(bool).values

dataset = MDPDataset(
    observations=obs,
    actions=actions,
    rewards=rewards,
    terminals=terminals,
)

# CQL ì„¤ì • (ì§€ì›ë˜ëŠ” ì¸ìë§Œ)
cfg = DiscreteCQLConfig(
    gamma=0.99,
    learning_rate=3e-4,
    batch_size=1024,
    encoder_factory="vector",
    q_func_factory="mean",
    n_critics=2,
    alpha=1.0,   # ë³´ìˆ˜ì„±
)

device = "cuda:0" if os.environ.get("CUDA_VISIBLE_DEVICES") else "cpu"
algo = DiscreteCQL(cfg, device, 0, False)  # (config, device, seed, enable_ddp)
echo(f"ì•Œê³ ë¦¬ì¦˜ ì¤€ë¹„ ì™„ë£Œ (device={device})")

try:
    # d3rlpy ìµœì‹  API: ë°ì´í„°ì…‹ ê¸°ë°˜ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ ì´ˆê¸°í™”
    algo.build_with_dataset(dataset)
    echo("ëª¨ë¸ ë¹Œë“œ ì™„ë£Œ: build_with_dataset()")
except Exception as ex:
    # ì¼ë¶€ ë²„ì „/í™˜ê²½ì—ì„œ build_with_dataset ë¯¸ë™ì‘ ì‹œ ì§ì ‘ ë¹Œë“œ í´ë°±
    try:
        obs_dim = obs.shape[1]
        action_size = int(np.max(actions)) + 1
        if hasattr(algo, "build"):
            algo.build(observation_shape=(obs_dim,), action_size=action_size)
            echo(f"ëª¨ë¸ ë¹Œë“œ ì™„ë£Œ: build(observation_shape={(obs_dim,)}, action_size={action_size})")
        else:
            raise
    except Exception as ex2:
        raise RuntimeError(
            f"ëª¨ë¸ ë¹Œë“œ ì‹¤íŒ¨: primary={type(ex).__name__}: {ex} / fallback={type(ex2).__name__}: {ex2}"
        )

# ğŸ” ë¡œê·¸ê°€ Model/d3rlpy_logs ì•„ë˜ì— ìƒì„±ë˜ë„ë¡ ì‘ì—… ë””ë ‰í„°ë¦¬ ì´ë™
os.chdir(MODEL_DIR)
echo(f"ì‘ì—… ë””ë ‰í„°ë¦¬ ì´ë™: {MODEL_DIR}")

# í•™ìŠµ ìŠ¤í…
N_STEPS = int(os.environ.get("TRAIN_STEPS", "30000"))
echo(f"í•™ìŠµ ì‹œì‘: n_steps={N_STEPS:,}, ë¡œê·¸ ë£¨íŠ¸={LOGS_ROOT}")

t0 = time.time()
# âœ… fitì—ëŠ” ì§€ì›ë˜ëŠ” ì¸ìë§Œ
algo.fit(
    dataset,
    n_steps=N_STEPS,
    n_steps_per_epoch=1000,
    save_interval=1000,
)
echo(f"í•™ìŠµ ì¢…ë£Œ: {(time.time()-t0):.1f}s")

# ì‚°ì¶œë¬¼ ì ê²€: Model/d3rlpy_logs/**/model_*.d3 ì¬ê·€ íƒìƒ‰
d3_list = sorted(LOGS_ROOT.rglob("model_*.d3"),
                 key=lambda p: int(p.stem.split("_")[-1]))
if not d3_list:
    raise RuntimeError("ì²´í¬í¬ì¸íŠ¸(.d3)ê°€ ìƒì„±ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ì“°ê¸° ê¶Œí•œ/ê²½ë¡œ í™•ì¸.")
latest = d3_list[-1]
echo(f"ìµœì‹  ì²´í¬í¬ì¸íŠ¸: {latest.relative_to(MODEL_DIR)} (size={latest.stat().st_size:,} bytes)")

print("í›ˆë ¨ ì™„ë£Œ. ì²´í¬í¬ì¸íŠ¸ëŠ” ì—¬ê¸°:", LOGS_ROOT)
