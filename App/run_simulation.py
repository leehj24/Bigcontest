# ==============================================================================
# self-bootstrap: `python run_simulation.py`ë¡œ ì‹¤í–‰í•´ë„ Streamlitë¡œ ì¬ì‹¤í–‰
# ==============================================================================
import os, sys, subprocess
from pathlib import Path

if __name__ == "__main__" and os.environ.get("STREAMLIT_BOOTSTRAPPED") != "1":
    file_path = str(Path(__file__).resolve())
    env = os.environ.copy()
    env["STREAMLIT_BOOTSTRAPPED"] = "1"
    cmd = [sys.executable, "-m", "streamlit", "run", file_path]
    subprocess.run(cmd, env=env, check=False)
    sys.exit(0)

# ==============================================================================
# ë³¸ ì•± ë¡œì§
# ==============================================================================
import streamlit as st
import pandas as pd
import numpy as np
import d3rlpy
import torch
import joblib
from pathlib import Path
from d3rlpy.algos import DiscreteCQL
from sklearn.preprocessing import StandardScaler

# --- PyTorch 2.6 í˜¸í™˜ ë˜í¼: d3rlpy ë‚´ë¶€ torch.loadê°€ weights_only=Trueë¡œ ê¹¨ì§€ëŠ” ë¬¸ì œ ë°©ì§€ ---
import torch as _torch
_torch_load_orig = _torch.load
def _torch_load_compat(*args, **kwargs):
    kwargs.setdefault("weights_only", False)  # êµ¬ë²„ì „ .d3 ì²´í¬í¬ì¸íŠ¸ í˜¸í™˜
    return _torch_load_orig(*args, **kwargs)
_torch.load = _torch_load_compat
# --------------------------------------------------------------------------------

# ì „ì—­ ìƒìˆ˜
LATENT_DIM = 16
FEATURES = ['DLV_SAA_RAT', 'MCT_UE_CLN_REU_RAT', 'MCT_UE_CLN_NEW_RAT', 'RC_M1_SAA_RANK', 'TREND_RATIO']

# ê²½ë¡œ ìœ í‹¸
def get_paths():
    app_dir = Path(__file__).resolve().parent
    root_dir = app_dir.parent  # Bigcontest
    model_dir = root_dir / "Model"
    eda_dir = root_dir / "EDA"
    logs_dir = model_dir / "d3rlpy_logs" / "DiscreteCQL"

    return {
        "ROOT": root_dir,
        "MODEL_DIR": model_dir,
        "EDA_DIR": eda_dir,
        "LOGS_DIR": logs_dir,
        "PARAMS_JSON": logs_dir / "params.json",
        "SCALER_PKL": model_dir / "standard_scaler.pkl",
        "NODE_EMB": model_dir / "node_embeddings.csv",
        "MERGED_CSV": eda_dir / "merged_data.csv",
        "RL_DATASET": model_dir / "rl_dataset.csv",
    }

# ìŠ¤ì¼€ì¼ëŸ¬ í™•ë³´(ì—†ìœ¼ë©´ ìë™ ìƒì„±) + ë­í¬ í•©ì„±ê¹Œì§€ ë‚´ì¥
def ensure_scaler(paths: dict) -> StandardScaler:
    scaler_path = paths["SCALER_PKL"]
    if scaler_path.exists():
        return joblib.load(scaler_path)

    # ë°ì´í„° ì†ŒìŠ¤ í™•ë³´
    if paths["MERGED_CSV"].exists():
        df = pd.read_csv(paths["MERGED_CSV"], encoding="utf-8-sig")
    elif paths["RL_DATASET"].exists():
        df = pd.read_csv(paths["RL_DATASET"])
    else:
        raise FileNotFoundError("standard_scaler.pkl ì—†ìŒ + EDA/merged_data.csv, Model/rl_dataset.csv ëª¨ë‘ ì—†ì–´ ìë™ ìƒì„± ë¶ˆê°€")

    # RC_M1_SAA_RANK ì—†ìœ¼ë©´ ì¦‰ì„ í•©ì„± (ì›”ë³„*ì—…ì¢…ë³„ ë§¤ì¶œ ê¸°ì¤€ 6êµ¬ê°„: 1=ìƒìœ„, 6=í•˜ìœ„)
    if "RC_M1_SAA_RANK" not in df.columns:
        required = {"RC_M1_SAA", "TA_YM", "HPSN_MCT_ZCD_NM"}
        if not required.issubset(df.columns):
            raise ValueError("ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ì‹¤íŒ¨: RC_M1_SAA_RANKê°€ ì—†ê³ , í•©ì„±ì— í•„ìš”í•œ ì—´(RC_M1_SAA, TA_YM, HPSN_MCT_ZCD_NM) ë¶€ì¡±")
        tmp = df.sort_values(["TA_YM", "HPSN_MCT_ZCD_NM", "RC_M1_SAA"], ascending=[True, True, False]).copy()
        tmp["__rank"] = tmp.groupby(["TA_YM", "HPSN_MCT_ZCD_NM"]).cumcount() + 1

        def to_bins(s):
            try:
                return pd.qcut(s, q=6, labels=[1,2,3,4,5,6], duplicates="drop").astype(int)
            except Exception:
                n = len(s)
                if n == 0:
                    return s.astype(int)
                edges = [(i*n)//6 for i in range(7)]
                idx = s.values - 1  # 0-index
                out = [min(max(sum(v >= e for e in edges[1:]) + 1, 1), 6) for v in idx]
                return pd.Series(out, index=s.index).astype(int)

        df["RC_M1_SAA_RANK"] = tmp.groupby(["TA_YM", "HPSN_MCT_ZCD_NM"])["__rank"].transform(to_bins)

    # ìŠ¤ì¼€ì¼ëŸ¬ í”¼íŒ…
    missing = [c for c in FEATURES if c not in df.columns]
    if missing:
        raise ValueError(f"ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ì‹¤íŒ¨: í•™ìŠµ í”¼ì²˜ ëˆ„ë½ {missing}")

    X = df[FEATURES].dropna()
    if len(X) == 0:
        raise ValueError("ìŠ¤ì¼€ì¼ëŸ¬ ìƒì„± ì‹¤íŒ¨: í•™ìŠµ ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŒ")

    scaler = StandardScaler().fit(X)
    scaler_path.parent.mkdir(parents=True, exist_ok=True)
    joblib.dump(scaler, scaler_path)
    return scaler

# d3rlpy DiscreteCQL ëª¨ë¸ì„ ê²¬ê³ í•˜ê²Œ ë¡œë“œ(ì—¬ëŸ¬ í›„ë³´ ì‹œë„)
def load_discrete_cql_with_fallback(params_path: Path, model_dir: Path, device: str):
    """
    ìš°ì„ ìˆœìœ„: model_10000.d3 -> 9000 -> ... -> 1000 -> cql_model.pt
    """
    algo = DiscreteCQL.from_json(params_path, device=device)

    logs_dir = model_dir / "d3rlpy_logs" / "DiscreteCQL"
    cand_steps = [10000, 9000, 8000, 7000, 6000, 5000, 4000, 3000, 2000, 1000]
    d3_candidates = [logs_dir / f"model_{s}.d3" for s in cand_steps]
    pt_candidate = model_dir / "cql_model.pt"

    tried = []

    for p in d3_candidates:
        if p.exists() and p.stat().st_size > 0:
            try:
                algo.load_model(p)  # d3rlpy native
                return algo, f"loaded: {p.name}"
            except Exception as ex:
                tried.append((str(p), str(ex)))

    if pt_candidate.exists() and pt_candidate.stat().st_size > 0:
        try:
            algo.load_model(pt_candidate)
            return algo, f"loaded (pt): {pt_candidate.name}"
        except Exception as ex:
            tried.append((str(pt_candidate), str(ex)))

    detail = "\n".join([f"- {p} :: {e}" for p, e in tried]) or "(no candidates tried)"
    raise RuntimeError("ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨. ì‹œë„ ë‚´ì—­:\n" + detail)

# í•„ìˆ˜ íŒŒì¼ í™•ì¸(ìµœì†Œ ìš”ê±´)
def assert_minimum_files(paths: dict):
    problems = []
    if not paths["NODE_EMB"].exists():
        problems.append(str(paths["NODE_EMB"]))
    if not paths["PARAMS_JSON"].exists():
        problems.append(str(paths["PARAMS_JSON"]))
    # ì²´í¬í¬ì¸íŠ¸ëŠ” ì—¬ëŸ¬ í›„ë³´ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í†µê³¼
    logs_dir = paths["LOGS_DIR"]
    has_any_ckpt = False
    if logs_dir.exists():
        for p in logs_dir.glob("model_*.d3"):
            if p.stat().st_size > 0:
                has_any_ckpt = True
                break
    pt_path = paths["MODEL_DIR"] / "cql_model.pt"
    if pt_path.exists() and pt_path.stat().st_size > 0:
        has_any_ckpt = True
    if not has_any_ckpt:
        problems.append(f"{logs_dir}\\model_*.d3 ë˜ëŠ” {pt_path} ì¤‘ í•˜ë‚˜")
    if problems:
        msg = "ë‹¤ìŒ í•„ìˆ˜ íŒŒì¼(ì¤‘ í•˜ë‚˜ ì´ìƒ)ì´ ì—†ìŠµë‹ˆë‹¤ ë˜ëŠ” ë¹„ì–´ ìˆìŠµë‹ˆë‹¤:\n- " + "\n- ".join(problems)
        raise FileNotFoundError(msg)

# AI ì „ëµê°€
class MarketQuantum:
    def __init__(self, model_dir: Path, params_path: Path, scaler: StandardScaler, node_data_path: Path):
        self.device = 'cuda:0' if torch.cuda.is_available() else 'cpu'
        # ê²¬ê³ í•œ ë¡œë” ì‚¬ìš©
        self.model, source_info = load_discrete_cql_with_fallback(params_path, model_dir, self.device)
        self.source_info = source_info

        self.scaler = scaler
        self.df_embeddings = pd.read_csv(node_data_path)

        self.action_map = {
            0: "ì „ëµ 0 (í˜„ìƒ ìœ ì§€ ë˜ëŠ” ë‚´ë¶€ ì—­ëŸ‰ ê°•í™”)",
            1: "ì „ëµ 1 (ì‹ ê·œ ê³ ê° ìœ ì¹˜ì— ì§‘ì¤‘)",
            2: "ì „ëµ 2 (ê¸°ì¡´ ê³ ê° ë˜ëŠ” ë°°ë‹¬ ì„œë¹„ìŠ¤ì— ì§‘ì¤‘)",
            3: "ì „ëµ 3 (ë°°ë‹¬ê³¼ ì‹ ê·œ ê³ ê° ëª¨ë‘ ê³µê²©ì ìœ¼ë¡œ í™•ì¥)",
        }

        self.latent_cols = [f'latent_{i}' for i in range(LATENT_DIM)]
        missing_latent = [c for c in self.latent_cols if c not in self.df_embeddings.columns]
        if missing_latent or 'ENCODED_MCT' not in self.df_embeddings.columns:
            need = missing_latent + (['ENCODED_MCT'] if 'ENCODED_MCT' not in self.df_embeddings.columns else [])
            raise ValueError(f"node_embeddings.csv ì»¬ëŸ¼ ëˆ„ë½: {need}")

    def get_state_vector(self, mct_id, monthly_data):
        row = self.df_embeddings[self.df_embeddings['ENCODED_MCT'] == mct_id]
        if row.empty:
            return None
        latent_vector = row[self.latent_cols].values[0].astype(np.float32)

        monthly_df = pd.DataFrame([monthly_data], columns=FEATURES)
        scaled_monthly = self.scaler.transform(monthly_df).astype(np.float32).flatten()

        state_vector = np.concatenate([latent_vector, scaled_monthly]).astype(np.float32)
        return state_vector

    def recommend_strategy(self, state_vector):
        if state_vector is None:
            return "ë¶„ì„ ì‹¤íŒ¨: ê°€ê²Œ IDë¥¼ ì„ë² ë”©ì—ì„œ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        action_idx = self.model.predict(np.expand_dims(state_vector, axis=0))[0]
        return self.action_map.get(int(action_idx), f"ì•Œ ìˆ˜ ì—†ìŒ(action={int(action_idx)})")

# ìºì‹œëœ ì—ì´ì „íŠ¸ ë¡œë”
@st.cache_resource
def load_ai_agent():
    paths = get_paths()
    assert_minimum_files(paths)
    scaler = ensure_scaler(paths)
    agent = MarketQuantum(
        model_dir=paths["MODEL_DIR"],
        params_path=paths["PARAMS_JSON"],
        scaler=scaler,
        node_data_path=paths["NODE_EMB"],
    )
    return agent

# Streamlit ì•±
st.set_page_config(page_title="ë§ˆì¼“ í€€í…€ AI ì „ëµ ì‹œë®¬ë ˆì´í„°", layout="wide")

# 1) ì—ì´ì „íŠ¸ ë¡œë”©
try:
    agent = load_ai_agent()
    all_store_ids = agent.df_embeddings['ENCODED_MCT'].dropna().unique()
except Exception as e:
    st.error(f"AI ì—ì´ì „íŠ¸ ë¡œë”© ì‹¤íŒ¨: {e}")
    st.stop()

# 2) ì œëª©/ì„¤ëª…
st.title("ğŸ“ˆ ë§ˆì¼“ í€€í…€: AI ì „ëµ ì‹œë®¬ë ˆì´í„°")
st.markdown("ì„±ë™êµ¬ ìƒê¶Œ ë””ì§€í„¸ íŠ¸ìœˆ ê¸°ë°˜ìœ¼ë¡œ ì í¬ë³„ **ë¯¸ë˜ ì„±ê³µ í™•ë¥ ì„ ë†’ì´ëŠ” ì „ëµ**ì„ ì œì•ˆí•œë‹¤.")
st.caption(
    f"Device: {'CUDA' if torch.cuda.is_available() else 'CPU'} / Policy: DiscreteCQL / {agent.source_info}"
)
st.divider()

# 3) ì‚¬ì´ë“œë°” ì…ë ¥
with st.sidebar:
    st.header("ğŸ” ì‹œë®¬ë ˆì´ì…˜ ì •ë³´ ì…ë ¥")
    target_store_id = st.selectbox(
        "ë¶„ì„í•  ê°€ê²Œ ID:",
        options=all_store_ids,
        index=0 if len(all_store_ids) else None
    )

    st.subheader("ê°€ì¥ ìµœê·¼ ì›” ì§€í‘œ ì…ë ¥")
    dlv_rat = st.slider("ë°°ë‹¬ ë§¤ì¶œ ë¹„ì¤‘ (%)", 0.0, 100.0, 10.5)
    reu_rat = st.slider("ì¬ë°©ë¬¸ ê³ ê° ë¹„ìœ¨ (%)", 0.0, 100.0, 30.2)
    new_rat = st.slider("ì‹ ê·œ ê³ ê° ë¹„ìœ¨ (%)", 0.0, 100.0, 15.8)
    saa_rank = st.select_slider("ë§¤ì¶œ ìˆœìœ„ êµ¬ê°„(1=ìƒìœ„)", options=[1, 2, 3, 4, 5, 6], value=4)
    trend_ratio = st.slider("í˜„ì¬ ì—…ì¢… íŠ¸ë Œë“œ ì§€ìˆ˜", 0.0, 100.0, 88.7)

    st.divider()
    run_button = st.button("AI ì „ëµ ë¶„ì„ ì‹œì‘", type="primary", use_container_width=True)

# 4) ê²°ê³¼
if run_button:
    latest_monthly_data = [dlv_rat, reu_rat, new_rat, saa_rank, trend_ratio]
    with st.spinner('AIê°€ ë””ì§€í„¸ íŠ¸ìœˆì—ì„œ ë°˜ì‚¬ì‹¤ì  ì‹œë‚˜ë¦¬ì˜¤ë¥¼ íƒìƒ‰ ì¤‘...'):
        state_vec = agent.get_state_vector(target_store_id, latest_monthly_data)
        recommendation = agent.recommend_strategy(state_vec)

    st.header("ğŸ“„ AI ì „ëµ ì‹œë®¬ë ˆì´ì…˜ ë¦¬í¬íŠ¸", divider='rainbow')
    col1, col2 = st.columns(2)
    with col1:
        st.metric(label="ë¶„ì„ ëŒ€ìƒ ê°€ê²Œ ID", value=str(target_store_id))
    with col2:
        st.metric(label="ë””ë°”ì´ìŠ¤", value="CUDA" if torch.cuda.is_available() else "CPU")

    st.subheader("âœ… AI ìµœì¢… ì „ëµ ì œì–¸")
    st.success(f"**{recommendation}**")

    st.info(
        "ê·¼ê±°: ì ì¬ ì„ë² ë”©(VGAE) + ìµœì‹  ì§€í‘œ(ë°°ë‹¬Â·ì¬ë°©ë¬¸Â·ì‹ ê·œÂ·ë§¤ì¶œìˆœìœ„Â·íŠ¸ë Œë“œ)ë¥¼ ê²°í•©í•´ "
        "ì˜¤í”„ë¼ì¸ RL(DiscreteCQL) ì •ì±…ìœ¼ë¡œ ì¥ê¸° ë³´ìƒ ê´€ì ì˜ ìµœì  í–‰ë™ì„ ì„ ì •.",
        icon="ğŸ’¡"
    )

# ì‚¬ìš© íŒ:
# - ì´ì œ `python App\run_simulation.py` í˜¹ì€ `python .\run_simulation.py`(App í´ë”ì—ì„œ) í•œ ì¤„ë¡œ ì‹¤í–‰ ê°€ëŠ¥
# - Gym ê²½ê³ ëŠ” ì •ë³´ì„±. í•„ìš”ì‹œ `pip install gymnasium`
